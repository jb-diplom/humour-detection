{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLM-Trainer","private_outputs":true,"provenance":[{"file_id":"1NEiqNPhiouu2pPwDAVeFoN4-vTYMz9F8","timestamp":1617477253720}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ccHJqOajNN7l"},"source":["#The Effect of Humour in Political Messaging: \n","An investigation combining fine-tuned neural language models and social network analysis<br>\n","by<br>\n","Janice Butler: University of Amsterdam, Master Thesis 2021"]},{"cell_type":"markdown","metadata":{"id":"DKpiNY516JUf"},"source":["## Introduction\n","This notebook implements the fine-tuning of various neural language models (NLMs) based on a new corpus of annotated humorous texts.\n","Two classifications are made\n","\n","1.   Degree of humour\n","2.   Comic styles\n","\n","## Method \n","The training script is modified from [run_glue.py](https://huggingface.co/transformers/examples.html#glue). \n","The training is automatically tracked in the Weights & Biases dashboard. \n","\n","### Supervised Fine-Tuning\n","\n","This script fine-tunes NMLs on corpora scraped from several sub-reddits:\n","* https://www.reddit.com/r/Jokes/\n","* https://www.reddit.com/r/satire/\n","* https://www.reddit.com/r/Showerthoughts/\n","* https://www.reddit.com/r/SurrealHumor\n","\n","and from twitter:\n","* https://twitter.com/midnight\n","\n","For non-humorous texts an equal amount of data was taken from these serious news outlets:\n","* https://twitter.com/AP\n","* https://twitter.com/BBCworld\n","* https://twitter.com/ITN\n","* https://twitter.com/ITVnews\n","* https://twitter.com/SkyNewsPolitics\n","* https://twitter.com/TheEconomist\n","\n","### Annotation\n","The reddit data is automatically annotated into 5 grades  according to the up-votes per subreddit. All other annotation was achieved through manual categorisation, for humour degree on the https://twitter.com/midnight tweets and in all cases for type of humour. The categories being:\n","* Serious\n","* Fun\n","* Benevolent humour\n","* Wit\n","* Nonsense\n","* Irony\n","* Satire\n","* Sarcasm\n","* Cynicism"]},{"cell_type":"markdown","metadata":{"id":"pt71k9Ew6Qp6"},"source":["## Install dependencies\n","\n","Pre-Trained Neural Language Models (NLMs) are taken from [the repository at Huggingface](https://huggingface.co/models). The generic Huggingface [Transformers API](https://huggingface.co/transformers/)  is used throughout for fine-tuning and the [Huggingface Pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html) is taken for easy utilisation of the finished models\n","\n","### NLM Training-Performance Monitoring\n","During fine-tuning a multitude of parameters are relayed to a data-base at https://wandb.ai/site. Additionally the fine-tuned model and all resultant meta-data for later cataloging of results and use with the model are recorded in projects defined [here](https://wandb.ai/jb-diplom) \n"," \n","\n","```\n","TODO: add screenshot loaded from GIT\n","```\n","\n","\n","\n","Install the Hugging Face transformers and Weights & Biases libraries, and the dataset and training script for humour fine-tuning."]},{"cell_type":"markdown","metadata":{"id":"_J05Pf8OnEyT"},"source":["## Installation and Import of Required Packages and Libraries\n","\n","The dependencies are as follows:\n","\n","\n","* Huggingface framework for loading and training models, preprocessing of data\n","* Optionally install transformers datasets, but not needed if own data/project data is being used\n","* Wandb is used for visualization of results on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n","* sentencepiece is required for deberta models\n","* General purpose libraries (os, glob, pandas, numpy)\n","* GUI and visualization libraries (data_table, ipywidgets, plotly, tqdm, matplotlib\n","* For calculating accuracy of fine-tuned models and visualizing the results , sklearn.metrics is used\n","\n"]},{"cell_type":"code","metadata":{"id":"mUP7zxN2NKUi","cellView":"form"},"source":["#@markdown Do imports\n","!pip install transformers -qq           # huggingface framework for loading and training models, preprocessing of data\n","# Uncomment following line to carry out benchmark tests with hf datasets\n","!pip install transformers datasets -qq  # currently transformers datasets --> add own data\n","!pip install wandb -qq                  # for visualization of results on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n","!pip install sentencepiece              # required for deberta\n","!pip install chart_studio\n","# this was the basis for the inital imlementation\n","# !wget https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py -qq\n","\n","# Weights and Biases logging of training metrics and archiving of training results\n","import wandb\n","\n","# General purpose libraries\n","from   google.colab import drive\n","import glob\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization libraries\n","%load_ext google.colab.data_table\n","from   google.colab import data_table\n","import ipywidgets as widgets\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from   tqdm.notebook import trange, tqdm\n","import matplotlib.pyplot as plt # For multi plots\n","\n","# Hugging face API for loading pre-trained models, fine-tuning and utilization\n","import transformers\n","from   transformers import AutoModelForSequenceClassification, AutoConfig, pipeline\n","\n","# Stuff for calculating accuracy of fine-tuned models\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import average_precision_score\n","from sklearn import metrics\n","\n","# Stuff for displaying metrics of fine-tuned models\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import RocCurveDisplay\n","from sklearn.metrics import PrecisionRecallDisplay"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bv5DsG_bALsh"},"source":["## API Key\n","The following calls registers this run at Weights and Biases github unless a session is already active.\n","Optionally, we can set environment variables to customize W&B logging. See [documentation](https://docs.wandb.com/library/integrations/huggingface).\n","\n","### Google Drive\n","The project data is hosted on GDrive to enable an easy interface with [Google Colab](https://colab.research.google.com/). The training data and results are taken from and stored to directories on GDrive, which has to be mounted and requires appropriate credentials."]},{"cell_type":"code","metadata":{"id":"a3FFmY4JqWdN"},"source":["#@markdown Connect to wandb\n","wandb.login()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-yWRxcu8kCB","cellView":"form"},"source":["#@markdown Mount GDrive\n","drive.mount('/content/gdrive',True)\n","file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/data/*\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"uB74DMmeMqq9"},"source":["#@markdown Set some global values for consistency of output styling\n","plot_bgcolor='rgb(150,150,160)'\n","cmap='viridis'\n","color_palette_r = px.colors.sequential.Viridis_r\n","color_palette   = px.colors.sequential.Viridis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nb4QBSswX1HV"},"source":["# Specify Parameters and Train Model"]},{"cell_type":"markdown","metadata":{"id":"TNWjjJYhNQx3"},"source":["Here you can choose which pre-trained NL model to fine-tune. Further options are:\n","\n","*   Which training-data to use\n","*   Which project to save run-time data to\n","*   The GLUE-Task to use\n","*   Initial learning rate\n","*   Number of epochs to train\n","*   Stepsize for logging\n","*   Whether to freeze layers\n","*   Testrun with mini dataset or not\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SYV7y-FvdAFT"},"source":["#@title Enter Parameters for Training { vertical-output: true, form-width: \"50%\", display-mode: \"form\" }\n","\n","#@markdown Specify Parameters for Training\n","#@markdown ---\n","\n","# Take viable names from https://huggingface.co/transformers/pretrained_models.html\n","Comment = \"distilbert-midnight_roughtest\" #@param {type:\"string\"}\n","Model = \"distilbert-base-uncased\" #@param [\"bert-base-uncased\", \"distilbert-base-uncased\", \"gpt2\", \"distilgpt2\", \"gpt2-medium\", \"xlnet-base-cased\", \"roberta-base\", \"distilroberta-base\", \"t5-base\", \"microsoft/deberta-base\", \"google/electra-base-discriminator\",\"google/electra-large-discriminator\", \"vinai/bertweet-base\"] {allow-input: true}\n","GLUE_Task = \"\" #@param [\"\",\"cola\", \"mnli\",\"mrpc\",\"qnli\",\"qqp\",\"rte\",\"sst2\",\"stsb\",\"wnli\"] {allow-input: false}\n","Initial_Learn_Rate = 2e-5 #@param {type: \"number\"}\n","\n","NrEpochs =   5#@param {type: \"number\"}\n","Do_Train = True #@param {type:\"boolean\"}\n","Do_Eval = True #@param {type:\"boolean\"}\n","Do_Predict = True #@param {type:\"boolean\"}\n"," \n","\n","#@markdown ---\n","#@markdown Parameters for Quick Tests\n","#@markdown ---\n","do_quick_test = True #@param {type:\"boolean\"}\n","Freeze_Layers = False #@param {type:\"boolean\"}\n","max_train_samples = 700 #@param {type:\"slider\", min:100, max:10000, step:100}\n","max_val_samples   = 60 #@param {type:\"slider\", min:10, max:1000, step:10}\n","max_test_samples  = 50 #@param {type:\"slider\", min:10, max:1000, step:10}\n","#Percent_of_Trainingdata_to_use = 10 #@param {type:\"slider\", min: 5, max:100, step:5}\n","\n","#@markdown ---\n","#@markdown Visualization Parameters\n","#@markdown ---\n","\n","Do_Visualization = True #@param {type:\"boolean\"}\n","WandB_Project = \"janice-roughtests\" #@param [\"thesis\", \"thesis-test-runs\"] {allow-input: true}\n","Logging_Steps = 20 #@param {type:\"slider\", min:10, max:100, step:10}\n","\n","#@markdown Choose Files for Training \n","#@markdown ---\n","file_ext = \".tsv\" #@param [\".tsv\", \".csv\", \".json\"] {allow-input: true}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhjzcbDMB9JC","cellView":"form"},"source":["#@markdown Which files would you like to use for traiing the NLM?\n","\n","dir_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/*/\")\n","dir_choice = widgets.Dropdown(options=dir_list,value=dir_list[0])\n","# file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/data/*\"+ file_ext)\n","# file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/msc_data/*\"+ file_ext)\n","file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/midnight/*\"+ file_ext)\n","\n","file_list.insert(0,\"\")\n","train_file = widgets.Dropdown(options=file_list,value=\"\")\n","validation_file = widgets.Dropdown(options=file_list,value=\"\")\n","test_file = widgets.Dropdown(options=file_list,value=\"\")\n","\n","items = [widgets.Label(value=\"Source Directory\"),\n","         widgets.Label(value= \"Training\"),\n","         widgets.Label(value=\"Validation\"),\n","         widgets.Label(value=\"Test\")]\n","\n","left_box = widgets.VBox([items[0], items[1], items[2], items[3]],width='10%')\n","right_box = widgets.VBox([dir_choice,train_file,validation_file,test_file],width='80%')\n","file_pickers=widgets.HBox([left_box, right_box], width='100%')\n","# file_pickers.overflow_x = 'auto'\n","right_box.overflow_x = 'auto'\n","\n","def updateDoclist(b):\n","    train_file.options=glob.glob(dir_choice.value + \"*train*\" + file_ext)\n","    validation_file.options=glob.glob(dir_choice.value + \"*dev*\" + file_ext)\n","    test_file.options=glob.glob(dir_choice.value + \"*test*\" + file_ext)\n","\n","dir_choice.observe(updateDoclist, names='value')\n","display(file_pickers)\n","updateDoclist(None)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pBSRh8kNRBi"},"source":["os.environ['TRAINING_FILE']=train_file.value\n","os.environ['VALID_FILE']=validation_file.value\n","os.environ['TEST_FILE']=test_file.value\n","os.environ['WANDB_PROJECT']=WandB_Project \n","os.environ['WANDB_LOG_MODEL'] = 'true'  # saving the model to wandb\n","os.environ['GLUE_TASK_NAME']=GLUE_Task\n","os.environ['TRAIN_EPOCHS']=str(NrEpochs)\n","os.environ['MODEL']=Model\n","os.environ['LR']=str(Initial_Learn_Rate)\n","os.environ['LS']=str(Logging_Steps)\n","os.environ['RUNNAME']=Comment\n","os.environ['REPORT_TO']=\"\"\n","os.environ['OUTPUT_DIR']=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model\n","os.environ['WANDB_WATCH']=\"all\"\n","os.environ['SAVE_STEPS']=\"5000\" # big step to avoid filling disk quota\n","os.environ['SAVE_LIMIT']=\"1\"    # only one backup (let's live dangeraously but save space)\n","os.environ['BATCH_SIZE']=\"12\"    \n","os.environ['SEQ_LENGTH']=\"256\"    \n","\n","\n","# to restart from checkpoint use following type of model path\n","# os.environ['MODEL']=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model+\"/checkpoint-12000/\"\n","if (Do_Visualization):\n","  os.environ['REPORT_TO']=\"wandb\"\n","\n","# %env\n","#  --task_name $GLUE_TASK_NAME \\\n","#  --jb_task_name \"t5\" \\\n","\n","if do_quick_test:\n","  os.environ['TRAIN_SAMPLES']=(str(max_train_samples) if do_quick_test else \"\")\n","  os.environ['VAL_SAMPLES']=  (str(max_val_samples)   if do_quick_test else \"\")\n","  os.environ['TEST_SAMPLES']= (str(max_test_samples)  if do_quick_test else \"\")\n","  \n","  !python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue2.py' \\\n","    --model_name_or_path $MODEL \\\n","    --max_val_samples $VAL_SAMPLES \\\n","    --max_test_samples $TEST_SAMPLES \\\n","    --max_train_samples $TRAIN_SAMPLES \\\n","    --tokenizer_name $MODEL \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --max_seq_length $SEQ_LENGTH \\\n","    --per_device_train_batch_size $BATCH_SIZE \\\n","    --per_device_eval_batch_size=64 \\\n","    --learning_rate $LR \\\n","    --num_train_epochs $TRAIN_EPOCHS \\\n","    --output_dir $OUTPUT_DIR \\\n","    --overwrite_output_dir \\\n","    --logging_steps $LS \\\n","    --run_name $RUNNAME \\\n","    --report_to $REPORT_TO \\\n","    --train_file $TRAINING_FILE \\\n","    --validation_file $VALID_FILE \\\n","    --test_file $TEST_FILE \\\n","    --save_steps $SAVE_STEPS \\\n","    --save_total_limit $SAVE_LIMIT \\\n","    --fp16 \\\n","    --adafactor --lr_scheduler_type cosine --warmup_ratio 0.1 \\\n","    --skip_memory_metrics\n","else:\n","  !python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue2.py' \\\n","    --model_name_or_path $MODEL \\\n","    --tokenizer_name $MODEL \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --max_seq_length $SEQ_LENGTH \\\n","    --per_device_train_batch_size $BATCH_SIZE \\\n","    --per_device_eval_batch_size=64 \\\n","    --learning_rate $LR \\\n","    --num_train_epochs $TRAIN_EPOCHS \\\n","    --output_dir $OUTPUT_DIR \\\n","    --overwrite_output_dir \\\n","    --logging_steps $LS \\\n","    --run_name $RUNNAME \\\n","    --report_to $REPORT_TO \\\n","    --train_file $TRAINING_FILE \\\n","    --validation_file $VALID_FILE \\\n","    --test_file $TEST_FILE \\\n","    --save_steps $SAVE_STEPS \\\n","    --save_total_limit $SAVE_LIMIT \\\n","    --fp16 \\\n","    --adafactor --lr_scheduler_type cosine --warmup_ratio 0.1 \\\n","    --skip_memory_metrics\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4FrJdV7eMAbG"},"source":["# Testing of fine-tuned models\n","Retrieve model from W&B repository"]},{"cell_type":"code","metadata":{"id":"ZKRacEzX6ifg"},"source":["# run = wandb.init()\n","run= wandb.init(project=WandB_Project, entity='jb-diplom')\n","# Take this from just finished run or from one of your \n","# favorite fine-tuned models at https://wandb.ai/jb-diplom/janice-full/artifacts\n","\n","#@markdown Enter Model Name to be retreived from https://wandb.ai\n","model_id = '1ap79mgp'  #@param {type: \"string\"}\n","\n","model_root= 'jb-diplom/' + WandB_Project + '/model-'\n","model_path= model_root + model_id + ':v0'\n","print(\"Retreiving artefact:\", model_path)\n","artifact = run.use_artifact(model_path, type='model')\n","# artifact = run.use_artifact('jb-diplom/janice-full/model-219xio3e:v0', type='model')\n","artifact_dir = artifact.download()\n","print(\"Model saved locally to:\", artifact_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQ9TBR-Q5Kg_"},"source":["print(\"Model saved locally to:\", artifact_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2jX4Te7yUMn","cellView":"form"},"source":["# Load fine-tuned model\n","# model_path=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model\n","# model_path=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/distilbert-base-uncased\"\n","\n","# model_path=\"/content/artifacts/model-39rcocd0:v0\" # A good bert-large with 0-3 labels\n","\n","model_path='/content/artifacts/model-' + model_id + ':v0' # The model just downloaded from wandb.io\n","\n","# This text classification pipeline can currently be loaded from pipeline() using the following task \n","# identifier: \"sentiment-analysis\"\n","\n","#@markdown Enter a test string to check whether the downloaded model is working\n","test_string = 'i shot a bullet into the air and it hit my hand. on one hand i\\u2019m really happy that it didn\\u2019t hit my head and kill me, but on the other hand i have a big gaping hole now.'  #@param {type: \"string\"}\n","\n","humour_classif = pipeline('sentiment-analysis',model_path)\n","humour_classif(test_string)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWfk3Nt4jDgD","cellView":"form"},"source":["#@markdown Load file (specified for testing above) for manual testing\n","# Take test.tsv and compare expected with actual results\n","test_df = pd.read_csv(test_file.value, delimiter='\\t', header=None, \n","                        lineterminator='\\n',encoding='utf-8')\n","cols=['Text','Humour Level']\n","test_df.columns=cols\n","# test_df.describe()\n","data_table.DataTable(test_df, include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IKgPZHCkjOM","cellView":"form"},"source":["# Do a little test against a chosen test-dataset\n","#@markdown #### **Attention:** This Operation may take many hours, depending on the quantity of test data!\n","#@markdown ----\n","#@markdown ###How many records would you like to test?\n","#@markdown An entry of -1 implies processing of **ALL** records\n","\n","sample_nr =   100#@param {type: \"number\"}\n","\n","# tweet_df.iloc[1:5, 1:1]\n","content=test_df.iloc[:,0]\n","labels=test_df.iloc[:,1]\n","humour=[]\n","y_true=[]\n","y_score=[]\n","\n","# establish humour content of tweet from fine-tuned model\n","hit=0\n","miss=0\n","out_by_one=0\n","\n","for i, tweet in tqdm(enumerate(content.head(sample_nr)),total=(len(content) if sample_nr==-1 else sample_nr)):\n","  # clip to max_seq_length, extract the number from the label of the result \n","  cls_val=humour_classif('{:1.512}'.format(tweet))\n","  val=cls_val[0]['label'][6:7:1]\n","  try:\n","    if (int(val) == int(labels[i][0:1:1])):\n","      y_true.append(1)\n","      hit +=1\n","    else:\n","      miss +=1\n","      y_true.append(0)\n","      if abs(int(val) - int(labels[i][0:1:1])) <3:\n","        out_by_one += 1\n","    humour.append(val)\n","    y_score.append(cls_val[0]['score'])\n","  except:\n","    continue\n","\n","print (\"Hits:\", hit,\"\\nMisses:\",miss,\"\\nOut by one:\", out_by_one, \"\\n%-age of hits:\", (hit*100)/(hit+miss))\n","# '{:1.35}'.format('12345678901234567890') use to truncate string\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Qny8fgXhozO"},"source":["#@title Analyse Results of Training\n","\n","#@markdown To assess the statistica success of the fine-tuning, 3 analyses are conducted using the test data-set:\n","#@markdown * Confusion Matrix<br>\n","#@markdown  **TODO** add descriptions \n","#@markdown * ROC<br>\n","#@markdown  **TODO** add descriptions \n","#@markdown * Precision Recall<br>\n","#@markdown  **TODO** add descriptions\n","#@markdown \n","\n","names=['serious','fun','benevolent','wit','nonsense','irony','satire','sarcasm','cynicism']\n","clean_h=[]\n","for val in humour:\n","  clean_h.append(val.replace('\\r',\"\"))\n","clean_p=[]\n","pred=test_df['Humour Level'].tolist()[1:len(clean_h)+1:1]\n","for val in pred:\n","  clean_p.append(val.replace(' ',\"\").replace('\\r',\"\"))\n","\n","cm=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5','6','7','8'], normalize= 'pred')\n","cm_display = ConfusionMatrixDisplay(cm, display_labels=names)\n","# fig, ax = plt.subplots(figsize=(10,10))\n","\n","fpr, tpr,  _ = roc_curve(y_true, y_score, pos_label=1)\n","roc_auc = metrics.auc(fpr, tpr)\n","# fig2, ax2 = plt.subplots(figsize=(10,10))\n","roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"\")\n","\n","avg_precision = average_precision_score(y_true, y_score)\n","prec, recall, _ = precision_recall_curve(y_true, y_score, pos_label=1)\n","pr_display = PrecisionRecallDisplay(precision=prec, recall=recall,average_precision=avg_precision,estimator_name=\"\" )\n","\n","# Try a multi plot :-)\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))\n","\n","cm_display.plot(ax=ax1,cmap='magma')\n","roc_display.plot(ax=ax2)\n","pr_display.plot(ax=ax3)\n","plt.show()\n","\n","# TODO : extend this to multi-class precision recall. See example here\n","# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCqnN2_JPx2l","cellView":"form"},"source":["#@title Save Results to GDrive\n","#@markdown The results are added as column ***Humour*** to the test (input) data and are saved with a suffix (the model name) as `.tsv` file in the same directory\n","\n","content=tweet_df.iloc[:,0]\n","humour=[]\n","\n","for tweet in tqdm(content.head(sample_nr)):\n","  humour.append(humour_classif(tweet)[0]['label'])\n","\n","hlen=len(humour)\n","for i in range (hlen, len(tweet_df)):\n","  humour.append(\"not evaluated\")\n","\n","# add humour column to dataframe\n","tweet_df['Humour']=humour\n","\n","# save results\n","tweet_file_out=basedir + file_name +'_' + model_id + ext\n","print (\"Saving to:\", tweet_file_out)\n","\n","# tweet_df.to_csv(tweet_file_out, sep='\\t', index=False, lineterminator='\\n',encoding='utf-16')\n","tweet_df.to_csv(tweet_file_out, sep='\\t', index=False,encoding='utf-16')\n","\n","data_table.DataTable(tweet_df.head(min(sample_nr,20000)), include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nn4Zr64i2ZI5"},"source":["# Read in tweets, do humour evaluation and write results back to df and save new csv\n","#@title Optionally Enter Alternative File Name for Conducting Test{ vertical-output: true, form-width: \"50%\", display-mode: \"form\" }\n","\n","#@markdown #### Specify Source for Humour Tests (skip this if you wish to continue with the results obtained from the previous step)\n","#@markdown ---\n","\n","basedir=\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/msc_data/\"#@param {type:\"string\"}\n","file_name='comedian-tweets250_2232a9v4'#@param {type:\"string\"}\n","ext=\".csv\" #@param {type:\"string\"}\n","tweet_file=basedir + file_name + ext\n","dtyps={'tweetId':str, 'content':str, 'username':str,'followers':int, 'conversationId':str, \n","         'replyCount':int, 'retweetCount':int, 'likeCount':int, 'quoteCount':int}\n","\n","# tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, dtype=dtyps,lineterminator='\\n',encoding='utf-16')\n","# tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, dtype=dtyps,encoding='utf-16')\n","# cols=['tweetId', 'content', 'username','followers', 'conversationId', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n","\n","tweet_df = pd.read_csv(tweet_file, delimiter='\\t',encoding='utf-16')\n","# tweet_df.pop(tweet_df.columns[0])\n","# tweet_df.pop(tweet_df.columns[3])\n","\n","tweet_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2I_h3Cw6sj5"},"source":["use_all_samples = True #@param {type:\"boolean\"}\n","sample_nr = 2660 #@param {type:\"slider\", min:10, max:5000, step:10}\n","output_max= sample_nr\n","\n","if use_all_samples:\n","  sample_nr = -1\n","  output_max = len(tweet_df)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gjo75aP7zVv6","collapsed":true,"cellView":"form"},"source":["#@title Display distribution of data for each type of humour\n","vc=tweet_df['Humour'].value_counts()\n","vc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"I-YiH3KZ7hgZ"},"source":["#@markdown Display resulting data\n","tweet_df = tweet_df.dropna()\n","subdf=tweet_df.query(\"Humour!='not evaluated' & likeCount > 5000\")\n","data_table.DataTable(subdf.head(20000), include_index=False, num_rows_per_page=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKLHUFSB96Oj"},"source":["# General purpose libraries\n","from   google.colab import drive\n","import glob\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization libraries\n","%load_ext google.colab.data_table\n","from   google.colab import data_table\n","import ipywidgets as widgets\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from   tqdm.notebook import trange, tqdm\n","import matplotlib.pyplot as plt # For multi plots"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nuKpOVydwTd"},"source":["#@title Choose Sample of Data for Visualization\n","\n","# limit of absolute_max=20000 due to colab DataTable\n","\n","import warnings\n","absolute_max=20000\n","\n","names=['serious','fun','benevolent','wit','nonsense','irony','satire','sarcasm','cynicism']\n","as_many_as_possible = False #@param {type:\"boolean\"}\n","sample = 5000 #@param {type:\"slider\", min:100, max:20000, step:100}\n","op_max= sample\n","\n","if as_many_as_possible:\n","  op_max = 20000\n","\n","likes_sorted=np.sort(tweet_df['likeCount'])\n","num_tweets=len(tweet_df)\n","divisor = min (absolute_max, op_max)\n","\n","last_like=0\n","if absolute_max < num_tweets:\n","  chunk_lst=np.array_split(likes_sorted,int(num_tweets/divisor))\n","  last_like=int(chunk_lst[-1][0])\n","\n","tweet_df = tweet_df.dropna()\n","subdf=tweet_df.loc[(tweet_df['Humour'] !='not evaluated') & (tweet_df['likeCount'] > last_like)]\n","\n","# add line breaks to make tooltips readable\n","with warnings.catch_warnings():\n","    warnings.simplefilter('ignore')\n","    subdf.content = subdf.content.str.wrap(80)\n","    subdf.content = subdf.content.apply(lambda x: x.replace('\\n', '<br>'))\n","\n","fig1 = px.scatter(subdf, x=\"retweetCount\", y=\"username\", size=\"likeCount\",\n","                 color=\"Humour\", hover_name=\"content\", facet_col=\"Humour\", size_max=100, log_x=True, \n","                 category_orders = {'Humour': ['LABEL_0','LABEL_1','LABEL_2','LABEL_3','LABEL_4','LABEL_5','LABEL_6','LABEL_7','LABEL_8']},\n","                 color_discrete_sequence=color_palette, height=3000,opacity=0.6, facet_col_wrap=3)\n","fig1.update_layout(\n","    plot_bgcolor=plot_bgcolor,\n","    title=\"Tweet Distribution per Humour Type amongst leading Twitterers\",\n","    xaxis_title=\"Retweets (log-scale)\",\n","    yaxis_title=\"Twitter Handles\",\n","  )\n","fig1.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"jyyBAXf12i2u"},"source":["#@title Calculate averages \n","#@markdown * For each type of Tweet propagation (reply, retweet, like, quote) \n","#@markdown * And each humour type\n","\n","table = pd.pivot_table(tweet_df, values=['replyCount', 'retweetCount', 'likeCount', 'quoteCount'], index=['Humour'],\n","                    aggfunc={'replyCount': np.mean,\n","                             'retweetCount': np.mean,\n","                             'likeCount': np.mean,\n","                             'quoteCount': np.mean\n","                             })\n","table=table.loc[(table.index != \"not evaluated\")]\n","\n","# need to rotate 'replyCount', 'retweetCount', 'likeCount', 'quoteCount' into one column and put the averages in a new column\n","dic ={\"humour\":[], \"propagation type\":[],\"mean value\":[]}\n","for htype in table.iterrows():\n","  for i in range (0,4) : dic[\"humour\"].append(htype[0])\n","  dic[\"propagation type\"].append('likes')\n","  dic[\"mean value\"].append(int(htype[1]['likeCount']))\n","  dic[\"propagation type\"].append('quotes')\n","  dic[\"mean value\"].append(int(htype[1]['quoteCount']))\n","  dic[\"propagation type\"].append('replies')\n","  dic[\"mean value\"].append(int(htype[1]['replyCount']))\n","  dic[\"propagation type\"].append('retweets')\n","  dic[\"mean value\"].append(int(htype[1]['retweetCount']))\n","\n","df_avg=pd.DataFrame(dic)\n","\n","df_avg.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"a9u4vMHxFgqb"},"source":["#@title Visualize average numbers of Tweet propagations per humour type\n","from plotly.graph_objs import *\n","\n","# labels={'Humour': ['LABEL_0','LABEL_1','LABEL_2','LABEL_3','LABEL_4','LABEL_5','LABEL_6','LABEL_7','LABEL_8'],\n","#                                       'propagation type': ['quotes','replies','retweets','likes']}\n","labels={'Humour': \"\",'propagation type': ''}\n","fig = px.histogram(df_avg, \n","                   x='propagation type', \n","                   y='mean value', \n","                   facet_col=\"humour\",\n","                   histfunc ='avg',\n","                   color_discrete_sequence=color_palette_r,\n","                   color='propagation type',\n","                  #  barmode='group',\n","                   category_orders = {'Humour': ['LABEL_0','LABEL_1','LABEL_2','LABEL_3','LABEL_4','LABEL_5','LABEL_6','LABEL_7','LABEL_8'],\n","                                      'propagation type': ['likes','retweets','replies','quotes']},\n","                   labels=labels\n","                   )\n","fig.update_xaxes(type='category')\n","\n","fig.update_layout(\n","    plot_bgcolor=plot_bgcolor,\n","    title=\"Average Numbers of Tweet Propagations per Humour Type\",\n","    xaxis_title=\"\",\n","    yaxis_title=\"Average Numbers of Tweet Propagations\",\n","    # legend_title=\"Legend Title\"\n","  )\n","fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"EWydwjixEcqU"},"source":["#@markdown ##Visualize Distribution of Tweets per Twitter-Handle\n","#@markdown Select/deselect humour types in the legend to analyse deeper\n","\n","subdf2=tweet_df.query(\"Humour!='not evaluated'\")\n","s = subdf2.groupby(\"username\")[\"followers\"].sum().rank(ascending=True)\n","# s = subdf2.groupby(\"username\").size().reset_index().groupby(['replyCount', 'retweetCount', 'likeCount', 'quoteCount']).sum().rank(ascending=True)\n","\n","\n","fig = px.histogram(subdf2, \n","                   x='username',\n","                   color=\"Humour\",\n","                   color_discrete_sequence=color_palette_r,\n","                   category_orders = {'Humour': ['LABEL_0','LABEL_1','LABEL_2','LABEL_3','LABEL_4','LABEL_5','LABEL_6','LABEL_7','LABEL_8'],\n","                                      'propagation type': ['likes','retweets','replies','quotes'],\n","                                      \"username\":s[s < 100000].sort_values().index.to_list()},\n","                   orientation='v', height=800\n","                      )\n","fig.update_layout(\n","    plot_bgcolor=plot_bgcolor,\n","    title=\"Numbers of Tweets amongst leading Twitterers\",\n","    )\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKqcZVuJZE3n"},"source":["# Neuer Abschnitt"]},{"cell_type":"markdown","metadata":{"id":"cM1m1eRHZFX5"},"source":["# Utility Code"]},{"cell_type":"code","metadata":{"id":"sOXYY86WVbWt"},"source":["# Invoke to show what gpu is in use\n","gpu = !nvidia-smi\n","gpu = '\\n'.join(gpu)\n","print(gpu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEBmDqVX97zk"},"source":["import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, datasets.ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsgNXvK3NF1d"},"source":["%env TRAINING_FILE\n","from datasets import load_dataset, load_metric\n","datasets = load_dataset( \"csv\", delimiter='\\t', data_files=[train_file.value, test_file.value,  validation_file.value])\n","\n","len(datasets)\n","datasets[\"train\"].column_names \n","datasets[\"train\"]\n","datasets[0]\n","show_random_elements(datasets,1)\n","train_file.value\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(train_file.value, delimiter='\\t', header=None, names=['sentence', 'label'])\n","\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hjtvx_Ydkj7"},"source":["%env\n","!python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue2.py' --help\n","train_file.value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFGc9Qs3A7AY"},"source":["## Visualization of results in dashboard\n","Analyze results (as they happen) on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n","\n","### To retrieve models and their metadata from wandb\n","\n","1.   Go to the artifacts area of wandb (e.g. `https://wandb.ai/jb-diplom/janice-full/artifacts`)\n","2.   Select the API Tag, which gives the precise code (below) for downloading the model that you need.\n","3.   Check in the artifacts folder of Colab for the sub-folder (e.g. `model-12ai5jvy:0`) with the model.<br> Right-click it and take a copy of the path\n","4.   Use the path in the huggingface pipeline constructor e.g.<br>\n","`model_path=\"/content/artifacts/model-15ai5jvy:v0\"`\n","`humour_classiffier = pipeline('sentiment-analysis',model_path)`\n"]},{"cell_type":"code","metadata":{"id":"GuKJtLLy3JM4"},"source":["!pip install jupyter-dash\n","\n","import plotly.express as px\n","from jupyter_dash import JupyterDash\n","import dash_core_components as dcc\n","import dash_html_components as html\n","from dash.dependencies import Input, Output\n","# Load Data\n","df = px.data.tips()\n","# Build App\n","app = JupyterDash(__name__)\n","app.layout = html.Div([\n","    html.H1(\"JupyterDash Demo\"),\n","    dcc.Graph(id='graph'),\n","    html.Label([\n","        \"colorscale\",\n","        dcc.Dropdown(\n","            id='colorscale-dropdown', clearable=False,\n","            value='plasma', options=[\n","                {'label': c, 'value': c}\n","                for c in px.colors.named_colorscales()\n","            ])\n","    ]),\n","])\n","# Define callback to update graph\n","@app.callback(\n","    Output('graph', 'figure'),\n","    [Input(\"colorscale-dropdown\", \"value\")]\n",")\n","def update_figure(colorscale):\n","    return px.scatter(\n","        df, x=\"total_bill\", y=\"tip\", color=\"size\",\n","        color_continuous_scale=colorscale,\n","        render_mode=\"webgl\", title=\"Tips\"\n","    )\n","# Run app and display result inline in the notebook\n","app.run_server(mode='inline')"],"execution_count":null,"outputs":[]}]}