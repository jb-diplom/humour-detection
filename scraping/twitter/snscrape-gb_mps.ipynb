{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pip install command below if you don't already have the library\n",
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryLists(file_name, hashtags=None,dates=\" until:2021-04-21 since:2010-01-01 \", min_faves=10, min_retweets=10):\n",
    "# returns a list of ca. 60 queries for the given hastags, dates and minimum virality which\n",
    "# in total cover queries for all UK MPs for these criteria\n",
    "# hastags format: \"(#covid OR #easter)\"\n",
    "# dates in the format: \" until:2021-03-01 since:2021-01-01 \"\n",
    "\n",
    "    mpSearches=getMPSearchStringList(file_name=file_name)\n",
    "    allQueries=[]\n",
    "    for i, mpSearch in enumerate(mpSearches) :\n",
    "        if hashtags != None:\n",
    "            allQueries.append(hashtags + mpSearch + dates + \" min_faves:\" + str(min_faves) + \" min_retweets:\" + str(min_retweets))\n",
    "        else:\n",
    "            allQueries.append(mpSearch + dates + \" min_faves:\" + str(min_faves) + \" min_retweets:\" + str(min_retweets))\n",
    "        \n",
    "    return allQueries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunks(file_name):\n",
    "    # Derived from the csv of all MPs from https://www.politics-social.com/list/followers\n",
    "    # a list of lists of twitter handles for all UK MPs is returned\n",
    "    \n",
    "    a_file = open(file_name, \"r\")\n",
    "    list_of_lists = []\n",
    "\n",
    "    for line in a_file:\n",
    "        stripped_line = line.strip()\n",
    "        list_of_lists.append(stripped_line)\n",
    "\n",
    "        splitlists=np.array_split(np.array(list_of_lists), 60) \n",
    "        # splitlists=np.array_split(np.array(list_of_lists), 10) \n",
    "\n",
    "    a_file.close()\n",
    "    return splitlists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMPSearchStringList(file_name = \"C:/Users/kbutl/OneDrive/janice/github/TwitterScraper/snscrape/python-wrapper/serious_news.txt\"):\n",
    "# returns a list of twitter query strings of the form \n",
    "# (from:zarahsultana OR from:Y_FovargueMP OR from:YvetteCooperMP OR from:YasminQureshiMP\n",
    "# the queries are limited in length due to Twitter restrictions, so a list of the queries\n",
    "# is needed. ca. 600 MPs --> list of ca. 60 queries\n",
    "\n",
    "    chunks=getChunks(file_name = file_name)\n",
    "    mp_search_strings = []\n",
    "    for i, chk in enumerate(chunks):\n",
    "        mp_search_strings.append(\"(from:\")\n",
    "        for j, mp in enumerate(chk):\n",
    "            if j!=0:\n",
    "                mp_search_strings[i] = mp_search_strings[i] + \" OR from:\" + mp\n",
    "            else:\n",
    "                mp_search_strings[i] = mp_search_strings[i] + mp\n",
    "        mp_search_strings[i] = mp_search_strings[i] +\")\"\n",
    "\n",
    "    return mp_search_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used for scraping below\n",
    "maxTweets = 250000          # set as required\n",
    "file_name = \"mps.txt\"       # set as required (see options below)\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "#  hashtags=None,dates=\" until:2021-03-01 since:2021-01-01 \", min_faves=10, min_retweets=10\n",
    "# file_name = \"journalists.txt\"):\n",
    "# file_name = \"serious_news.txt\"\n",
    "# file_name = \"comedians.txt\"\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list1 = []\n",
    "\n",
    "ql=getQueryLists(file_name)\n",
    "\n",
    "for mpQs in getQueryLists(file_name):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(mpQs).get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        tweets_list1.append([str(tweet.id), tweet.content, tweet.user.username,tweet.user.followersCount, str(tweet.conversationId),        tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount],  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some serious tweets, set the label to 0 (no humour)\n",
    "# Setting variables to be used below\n",
    "maxTweets = 200000\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list0 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "#  hashtags=None,dates=\" until:2021-03-01 since:2021-01-01 \", min_faves=10, min_retweets=10\n",
    "file_name = \"C:/Users/kbutl/OneDrive/janice/github/TwitterScraper/snscrape/python-wrapper/serious_news.txt\"\n",
    "\n",
    "for mpQs in getQueryLists(file_name, min_faves=0,min_retweets=0):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(mpQs).get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        tweets_list0.append([\"0\", tweet.content],  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the serious tweets list above\n",
    "tweets_df0 = pd.DataFrame(tweets_list0, columns=['label', 'sentence'])\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               tweetId                                            content  \\\n",
       "0  1384617849415688192  American police forces were established to cat...   \n",
       "1  1384613550526803969  Oh my god, I sound so horny as a French person...   \n",
       "2  1384597084448464906      Take down immediatejt https://t.co/CiVIQEfo1V   \n",
       "3  1384498297381945346                            https://t.co/6jKt6gC87Z   \n",
       "4  1384482731464138758  Susan's #CatofTheDay is Happ.\\n\\nThank you @Fr...   \n",
       "\n",
       "      username  followers       conversationId  replyCount  retweetCount  \\\n",
       "0   robdelaney    1581288  1384617849415688192          78           455   \n",
       "1   WeeMissBea     508964  1384613550526803969          55            21   \n",
       "2   robdelaney    1581288  1384597084448464906          17            46   \n",
       "3   robdelaney    1581288  1384498297381945346          34            32   \n",
       "4  SusanCalman     260039  1384482731464138758          15            24   \n",
       "\n",
       "   likeCount  quoteCount  \n",
       "0       4774          10  \n",
       "1       1016           1  \n",
       "2       1702           1  \n",
       "3        623           5  \n",
       "4       1138           1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweetId</th>\n      <th>content</th>\n      <th>username</th>\n      <th>followers</th>\n      <th>conversationId</th>\n      <th>replyCount</th>\n      <th>retweetCount</th>\n      <th>likeCount</th>\n      <th>quoteCount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1384617849415688192</td>\n      <td>American police forces were established to cat...</td>\n      <td>robdelaney</td>\n      <td>1581288</td>\n      <td>1384617849415688192</td>\n      <td>78</td>\n      <td>455</td>\n      <td>4774</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1384613550526803969</td>\n      <td>Oh my god, I sound so horny as a French person...</td>\n      <td>WeeMissBea</td>\n      <td>508964</td>\n      <td>1384613550526803969</td>\n      <td>55</td>\n      <td>21</td>\n      <td>1016</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1384597084448464906</td>\n      <td>Take down immediatejt https://t.co/CiVIQEfo1V</td>\n      <td>robdelaney</td>\n      <td>1581288</td>\n      <td>1384597084448464906</td>\n      <td>17</td>\n      <td>46</td>\n      <td>1702</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1384498297381945346</td>\n      <td>https://t.co/6jKt6gC87Z</td>\n      <td>robdelaney</td>\n      <td>1581288</td>\n      <td>1384498297381945346</td>\n      <td>34</td>\n      <td>32</td>\n      <td>623</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1384482731464138758</td>\n      <td>Susan's #CatofTheDay is Happ.\\n\\nThank you @Fr...</td>\n      <td>SusanCalman</td>\n      <td>260039</td>\n      <td>1384482731464138758</td>\n      <td>15</td>\n      <td>24</td>\n      <td>1138</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['tweetId', 'content', 'username','followers', 'conversationId', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount'])\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "# tweets_df1.to_csv('user-tweets.csv', sep='\\t', index=False, encoding=\"utf-16\")\n",
    "\n",
    "tweets_df1.to_csv('comedian-tweets250.csv', sep='\\t', index=False, encoding=\"utf-16\")\n",
    "# tweets_df1.to_csv('user-tweets.csv', sep='\\t', index=False, encoding=\"utf-8-sig\")\n",
    "# tweets_df1.to_excel('xuser-tweets.xlsx',  index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data\n",
    "tweets_df0[\"sentence\"] = tweets_df0[\"sentence\"].replace(r'\\n',' ',regex=True)\n",
    "tweets_df0[\"sentence\"] = tweets_df0[\"sentence\"].replace(r'\\t',' ',regex=True)\n",
    "tweets_df0[\"sentence\"] = tweets_df0[\"sentence\"].replace(r'\\r',' ',regex=True)\n",
    "tweets_df0 = tweets_df0.dropna()\n",
    "\n",
    "# tweets_df0.to_csv('serious_tweets.tsv', sep='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\', index=None, header=['label','sentence']\n",
    "tweets_df0.to_csv('serious_tweets.tsv', sep='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\', index=None, header=None,encoding=\"UTF-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe from csv\n",
    "import pandas as pd\n",
    "tweet_file=\"user-tweets.csv\"\n",
    "columns_in=['tweetId', 'content', 'username','followers', 'conversationId', \n",
    "         'replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n",
    "tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, names=columns_in, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Text Search\n",
    "The code below will scrape for 500 tweets between June 1st, 2020 and July 31st, 2020, by a text search then provide a CSV file with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used below\n",
    "maxTweets = 50000\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('its the elephant since:2020-06-01 until:2020-07-31').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df1.to_csv('text-query-tweets.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mayavi\n",
    "import networkx as nx\n",
    "\n",
    "from mayavi import mlab\n",
    "\n",
    "H = nx.from_pandas_edgelist(tweets_df1, 'Tweet Id','conversationId') \n",
    "# reorder nodes from 0,len(G)-1\n",
    "G = nx.convert_node_labels_to_integers(H)\n",
    "# 3d spring layout\n",
    "pos = nx.spring_layout(G, dim=3)\n",
    "# pos = nx.random_layout(G, dim=3)\n",
    "# numpy array of x,y,z positions in sorted node order\n",
    "xyz = np.array([pos[v] for v in sorted(G)])\n",
    "# scalar colors\n",
    "scalars = np.array(list(G.nodes())) + 5\n",
    "\n",
    "pts = mlab.points3d(\n",
    "    xyz[:, 0],\n",
    "    xyz[:, 1],\n",
    "    xyz[:, 2],\n",
    "    scalars,\n",
    "    scale_factor=0.1,\n",
    "    scale_mode=\"none\",\n",
    "    colormap=\"Blues\",\n",
    "    resolution=20,\n",
    ")\n",
    "\n",
    "pts.mlab_source.dataset.lines = np.array(list(G.edges()))\n",
    "tube = mlab.pipeline.tube(pts, tube_radius=0.01)\n",
    "mlab.pipeline.surface(tube, color=(0.8, 0.8, 0.8))\n",
    "# mlab.init_notebook()\n",
    "mlab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reread tweetfile\n",
    "\n",
    "dtyps={'tweetId':str, 'content':str, 'username':str,'followers':int, 'conversationId':str, \n",
    "         'replyCount':int, 'retweetCount':int, 'likeCount':int, 'quoteCount':int}\n",
    "\n",
    "tweet_df = pd.read_csv('user-tweets.csv', delimiter='\\t', header=None, dtype=dtyps,\n",
    "                        lineterminator='\\n',encoding='utf-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\kbutl\\OneDrive\\janice\\github\\rJokes\\rJokesData\\data\\tweets_w_humour.tsv\n",
    "# rJokes\\rJokesData\\data\\tweets_w_humour.tsv\n",
    "# C:\\Users\\kbutl\\OneDrive\\janice\\github\\TwitterScraper\\snscrape\\python-wrapper\\user-tweets.csv\n",
    "# C:\\Users\\kbutl\\OneDrive\\janice\\github\\rJokes\\rJokesData\\data\\tweets_w_humour_korr.tsv\n",
    "\n",
    "dtyps={'tweetId':str, 'content':str, 'username':str,'followers':int, 'conversationId':str, \n",
    "         'replyCount':int, 'retweetCount':int, 'likeCount':int, 'quoteCount':int, 'Humour':str}\n",
    "\n",
    "# tw_df_humour = pd.read_csv('../../../TwitterScraper/snscrape/python-wrapper/tweets_w_humour_korr.tsv', delimiter='\\t', header=None, dtype=dtyps,\n",
    "#                         lineterminator='\\n',encoding='utf-16')\n",
    "tw_df_journalists = pd.read_csv('../../../TwitterScraper/snscrape/python-wrapper/jtweets_w_humour_distilbert_balanced.tsv', delimiter='\\t', dtype=dtyps,lineterminator='\\n',encoding='utf-16')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtyps={'tweetId':str, 'content':str, 'username':str,'followers':int, 'conversationId':str, \n",
    "         'replyCount':int, 'retweetCount':int, 'likeCount':int, 'quoteCount':int, 'Humour':str}\n",
    "\n",
    "# tw_df_humour = pd.read_csv('../../../rJokes/rJokesData/data/tweets_w_humour_korr.tsv', delimiter='\\t', header=None, dtype=dtyps,\n",
    "#                         lineterminator='\\n',encoding='utf-16')\n",
    "tw_df_humourdistilbert120k = pd.read_csv('../../../rJokes/rJokesData/data/tweets_w_humour_distilbert120k.tsv', delimiter='\\t', dtype=dtyps,lineterminator='\\n',encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing train, dev, and test with shapes (18340, 2) (2292, 2) (2293, 2)\n"
     ]
    }
   ],
   "source": [
    "# Wrangle the midnight files (HUMOUR Type) into shape for training, adding serious stuff in equal measure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_midnight = pd.read_csv('../../../rJokes/rJokesData/data_annot/annotatedX.tsv', sep='\\t', header=None,names=['sentence','label0','labela','label'],encoding='utf-8')\n",
    "\n",
    "df_midnight.pop(df_midnight.columns[2])     # remove humour degree column\n",
    "df_midnight.pop(df_midnight.columns[1])     # remove original midnight scores (0,1,2)\n",
    "df_midnight = df_midnight.dropna()\n",
    "\n",
    "\n",
    "df_satire = pd.read_csv('../../../rJokes/rJokesData/data_annot/reddit_satire.tsv', sep='\\t', header=None,names=['label','sentence','label0'],encoding='utf-8')\n",
    "df_satire.pop(df_satire.columns[0])     # remove humour degree column\n",
    "df_satire = df_satire.dropna()\n",
    "\n",
    "# read in non-humourous data\n",
    "file_in='../../../TwitterScraper/snscrape/python-wrapper/serious_tweets.tsv'\n",
    "serious = pd.read_csv(file_in, sep='\\t', header=None, names=['label','sentence'], encoding=\"UTF-8\")\n",
    "# serious = pd.read_csv(file_in, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\', header=None, names=['label','sentence'], encoding=\"UTF-8\",lineterminator='\\n')\n",
    "serious = serious.replace([np.inf, -np.inf], np.nan)\n",
    "serious = serious.dropna()\n",
    "columns_titles = [\"sentence\",\"label\"]\n",
    "\n",
    "serious=serious.reindex(columns=columns_titles)\n",
    "df_satire=df_satire.reindex(columns=columns_titles)\n",
    "\n",
    "df_midnight =pd.concat([df_midnight, df_satire])                                            # add in the satire\n",
    "df_midnight =pd.concat([df_midnight, serious[:min(int(len(df_midnight)),len(serious))]])    # add in the serious\n",
    "\n",
    "# clean up where data's missing or wrong due to concatenations\n",
    "df_midnight = df_midnight.dropna()\n",
    "\n",
    "# clean up special characters\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\n',' ',regex=True)\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\t',' ',regex=True)\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\r',' ',regex=True)\n",
    "df_midnight[\"label\"] = df_midnight[\"label\"].replace(r'\\r','',regex=True)\n",
    "df_midnight[\"label\"] = df_midnight[\"label\"].replace(r' ','',regex=True)\n",
    "\n",
    "df_midnight = df_midnight[df_midnight['sentence'] != 'sentence']    #remove all rows containing \"sentence\"\n",
    "assert df_midnight[\"sentence\"].isnull().any() == False, \"there were NaNs in the sentence column, ERROR\"\n",
    "assert df_midnight[\"label\"].isnull().any() == False, \"there were NaNs in the score column, ERROR\"\n",
    "\n",
    "num_degrees=8   # Humour type has 0-8 possibilities\n",
    "subdir_name='../../TwitterScraper/snscrape/python-wrapper'\n",
    "\n",
    "train, dev = train_test_split(df_midnight, test_size=0.2, random_state=42)\n",
    "dev, test = train_test_split(dev, test_size=0.5, random_state=42)\n",
    "print(\"writing train, dev, and test with shapes\", train.shape, dev.shape, test.shape)\n",
    "suffix=\"_htype_0to\" + str(num_degrees)\n",
    "train.to_csv(os.path.join(\"..\", subdir_name, \"train\"+suffix+\".tsv\"), sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "dev.to_csv(os.path.join(\"..\", subdir_name, \"dev\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "test.to_csv(os.path.join(\"..\", subdir_name, \"test\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing train, dev, and test with shapes (19983, 2) (2498, 2) (2498, 2)\n"
     ]
    }
   ],
   "source": [
    "# Wrangle the midnight files (HUMOUR degree) into shape for training, adding serious stuff in equal measure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_midnight = pd.read_csv('../../../rJokes/rJokesData/data_annot/annotatedX.tsv', sep='\\t', header=None,names=['sentence','label0','labela','label'],encoding='utf-8')\n",
    "\n",
    "df_midnight.pop(df_midnight.columns[1])     # remove original midnight scores (0,1,2)\n",
    "df_midnight.pop(df_midnight.columns[2])     # remove humour degree column\n",
    "df_midnight = df_midnight.dropna()\n",
    "\n",
    "columns_titles = [\"sentence\",\"label\"]\n",
    "\n",
    "df_satire = pd.read_csv('../../../rJokes/rJokesData/data_annot/reddit_satire.tsv', sep='\\t', header=None,names=['label','sentence','label0'],encoding='utf-8')\n",
    "df_satire.pop(df_satire.columns[2])     # remove humour degree column\n",
    "df_satire = df_satire.dropna()\n",
    "\n",
    "# read in non-humourous data\n",
    "file_in='../../../TwitterScraper/snscrape/python-wrapper/serious_tweets.tsv'\n",
    "serious = pd.read_csv(file_in, sep='\\t', header=None, names=['label','sentence'], encoding=\"UTF-8\")\n",
    "# serious = pd.read_csv(file_in, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\', header=None, names=['label','sentence'], encoding=\"UTF-8\",lineterminator='\\n')\n",
    "serious = serious.replace([np.inf, -np.inf], np.nan)\n",
    "serious = serious.dropna()\n",
    "\n",
    "serious=serious.reindex(columns=columns_titles) # Swap order of columns\n",
    "df_satire=df_satire.reindex(columns=columns_titles)\n",
    "df_midnight.columns=columns_titles # correct column names\n",
    "\n",
    "df_midnight =pd.concat([df_midnight, df_satire])                                            # add in the satire\n",
    "df_midnight =pd.concat([df_midnight, serious[:min(int(len(df_midnight)),len(serious))]])    # add in the serious\n",
    "\n",
    "# clean up where data's missing or wrong due to concatenations\n",
    "df_midnight = df_midnight.dropna()\n",
    "\n",
    "# clean up special characters\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\n',' ',regex=True)\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\t',' ',regex=True)\n",
    "df_midnight[\"sentence\"] = df_midnight[\"sentence\"].replace(r'\\r',' ',regex=True)\n",
    "df_midnight[\"label\"] = df_midnight[\"label\"].replace(r'\\r','',regex=True)\n",
    "df_midnight[\"label\"] = df_midnight[\"label\"].replace(r' ','',regex=True)\n",
    "\n",
    "df_midnight = df_midnight[df_midnight['sentence'] != 'sentence']    #remove all rows containing \"sentence\"\n",
    "assert df_midnight[\"sentence\"].isnull().any() == False, \"there were NaNs in the sentence column, ERROR\"\n",
    "assert df_midnight[\"label\"].isnull().any() == False, \"there were NaNs in the score column, ERROR\"\n",
    "\n",
    "num_degrees=5   # Humour degree has 0-5 possibilities\n",
    "subdir_name='../../TwitterScraper/snscrape/python-wrapper'\n",
    "\n",
    "train, dev = train_test_split(df_midnight, test_size=0.2, random_state=42)\n",
    "dev, test = train_test_split(dev, test_size=0.5, random_state=42)\n",
    "print(\"writing train, dev, and test with shapes\", train.shape, dev.shape, test.shape)\n",
    "suffix=\"_hdeg_0to\" + str(num_degrees)\n",
    "train.to_csv(os.path.join(\"..\", subdir_name, \"train\"+suffix+\".tsv\"), sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "dev.to_csv(os.path.join(\"..\", subdir_name, \"dev\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "test.to_csv(os.path.join(\"..\", subdir_name, \"test\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing train, dev, and test with shapes (25371, 2) (3171, 2) (3172, 2)\n"
     ]
    }
   ],
   "source": [
    "# Just split give tsv's into train, tes, dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dtyps={'sentence':str, 'label':str}\n",
    "\n",
    "# tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, dtype=dtyps,lineterminator='\\n',encoding='utf-16')\n",
    "\n",
    "df_deg = pd.read_csv('../../data-training/complete/htypes.tsv', sep='\\t', dtype=dtyps,encoding='utf-8')\n",
    "# df_deg = pd.read_csv('../../data-training/complete/hdeg.tsv', sep='\\t', header=None,names=['sentence','label'],encoding='utf-8')\n",
    "\n",
    "# columns_titles = [\"sentence\",\"label\"]\n",
    "\n",
    "df_deg = df_deg.dropna()\n",
    "\n",
    "# clean up special characters\n",
    "df_deg[\"sentence\"] = df_deg[\"sentence\"].replace(r'\\n',' ',regex=True)\n",
    "df_deg[\"sentence\"] = df_deg[\"sentence\"].replace(r'\\t',' ',regex=True)\n",
    "df_deg[\"sentence\"] = df_deg[\"sentence\"].replace(r'\\r',' ',regex=True)\n",
    "df_deg[\"label\"] = df_deg[\"label\"].replace(r'\\r','',regex=True)\n",
    "df_deg[\"label\"] = df_deg[\"label\"].replace(r' ','',regex=True)\n",
    "\n",
    "# df_midnight = df_midnight[df_midnight['sentence'] != 'sentence']    #remove all rows containing \"sentence\"\n",
    "assert df_deg[\"sentence\"].isnull().any() == False, \"there were NaNs in the sentence column, ERROR\"\n",
    "assert df_deg[\"label\"].isnull().any() == False, \"there were NaNs in the score column, ERROR\"\n",
    "\n",
    "num_degrees=8   # Humour degree has 0-5 possibilities\n",
    "subdir_name='../data-training/complete/'\n",
    "\n",
    "train, dev = train_test_split(df_deg, test_size=0.2, random_state=42)\n",
    "dev, test = train_test_split(dev, test_size=0.5, random_state=42)\n",
    "print(\"writing train, dev, and test with shapes\", train.shape, dev.shape, test.shape)\n",
    "suffix=\"_htype_balanced_0to\" + str(num_degrees)\n",
    "train.to_csv(os.path.join(\"..\", subdir_name, \"train\"+suffix+\".tsv\"), sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "dev.to_csv(os.path.join(\"..\", subdir_name, \"dev\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")\n",
    "test.to_csv(os.path.join(\"..\", subdir_name, \"test\"+suffix+\".tsv\"),  sep=\"\\t\",  escapechar='\\\\', index=None, header=['sentence','label'], encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0130ab9669a05b4bd668763f15ee24134ef0f142e9716cb22deddc76245e78025",
   "display_name": "Python 3.8.8 64-bit ('pyt': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "ffb253afe4fcb2cc997351f06f6b04c7afa08be021a74500475df96cbbc247ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}